# =============================================================================
# Robots.txt - YAA Project Search Engine Optimization
# Directives for web crawlers and search engine bots
# Last updated: 2025
# =============================================================================

# =============================================================================
# Default crawler rules - applies to all user agents
# =============================================================================
User-agent: *
Allow: /
Disallow: /private/
Disallow: /admin/
Disallow: /temp/
Disallow: /cache/
Disallow: /uploads/
Disallow: /.git/
Disallow: /.gitignore
Disallow: /node_modules/
Disallow: /src/
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /*?*sort=
Disallow: /*?*order=
Disallow: /*?*page=
Disallow: /?print
Disallow: /?mobile
Disallow: /search/
Disallow: /*?
Allow: /$
Allow: /index.html$
Allow: /*.css$
Allow: /*.js$
Allow: /*.png$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.gif$
Allow: /*.webp$
Allow: /*.svg$
Allow: /*.ico$
Allow: /*.woff$
Allow: /*.woff2$
Allow: /*.ttf$

# Crawl-delay for responsible crawling
Crawl-delay: 1
Request-rate: 30/60

# =============================================================================
# Google (Googlebot) - Standard crawling rules
# =============================================================================
User-agent: Googlebot
Allow: /
Disallow: /private/
Disallow: /admin/
Crawl-delay: 0

# Google Image Bot
User-agent: Googlebot-Image
Allow: /
Disallow: /private/

# Google Mobile Bot
User-agent: Googlebot-Mobile
Allow: /
Disallow: /private/

# =============================================================================
# Bing (Bingbot) - Standard crawling rules
# =============================================================================
User-agent: Bingbot
Allow: /
Disallow: /private/
Disallow: /admin/
Crawl-delay: 1

# =============================================================================
# Yahoo (Slurp) - Standard crawling rules
# =============================================================================
User-agent: Slurp
Allow: /
Disallow: /private/
Disallow: /admin/
Crawl-delay: 1

# =============================================================================
# DuckDuckGo - Standard crawling rules
# =============================================================================
User-agent: DuckDuckBot
Allow: /
Disallow: /private/
Disallow: /admin/
Crawl-delay: 1

# =============================================================================
# Block unwanted crawlers and bots
# =============================================================================

# Block AI training bots
User-agent: ChatGPT-User
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

# Block MJ12bot (Majestic)
User-agent: MJ12bot
Allow: /robots.txt
Allow: /sitemap.xml
Disallow: /

# Block AhrefsBot
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

# Block aggressive crawlers
User-agent: BadBot
Disallow: /

User-agent: HeadlessChrome
Disallow: /

User-agent: selenium
Disallow: /

User-agent: curl
Disallow: /

# =============================================================================
# Sitemap and Additional Resources
# =============================================================================
Sitemap: https://yaa.group/sitemap.xml
Sitemap: https://yaa.group/sitemap-index.xml

# =============================================================================
# Notes
# =============================================================================
# - This robots.txt controls crawler access to the website
# - Search engines should respect the directives specified here
# - For more information: https://developers.google.com/search/docs/advanced/robots/robots_txt
# - Test robots.txt at: https://www.google.com/webmasters/tools/robots-testing-tool
# - Ensure sitemap.xml is updated and accessible
# =============================================================================
